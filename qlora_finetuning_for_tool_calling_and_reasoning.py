# -*- coding: utf-8 -*-
"""Qlora_Finetuning_for_tool_calling_and_reasoning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hbyBaXK1IbebUnNbqUM_irhUptjyxtoD
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install -U datasets
# !pip install -U trl
# !pip install -U transformers accelerate
# !pip install -U bitsandbytes
# # # # # # # uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
# from dotenv import load_dotenv
import os
from peft import LoraConfig
from peft import TaskType
from trl import SFTConfig, SFTTrainer
from transformers import BitsAndBytesConfig
# load_dotenv()

from huggingface_hub import login
login()
print("torch version: ", torch.__version__)
print('is gpu enbled: ', torch.cuda.is_available())

compute_dtype = getattr(torch, "float16")
bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type='nf4',
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=False,
    )

# Load model and tokenizer
model_id = "google/gemma-2b-it"

tokenizer = AutoTokenizer.from_pretrained(model_id)
# base_model = AutoModelForCausalLM.from_pretrained(model_id,
#                                                   quantization_config=bnb_config,
#                                                   device_map='auto')
# meta-llama/Llama-3.2-3B-Instruct

# for name, weights in base_model.named_parameters():
#     print(name)
#     print(weights)
#     break

# Load the dataset
from datasets import load_dataset
ds = load_dataset("Jofthomas/hermes-function-calling-thinking-V1")

ds['train'][0]

base_model

# ds['train'][12] # sample data

sample_data = ds['train'][0]['conversations']

# we won't use bos token in here , since it's will be added when we set SFTTrainer and data get tokenized, so we set new chat template there with bos token and no other difference
chat_template_for_preprocessing = "{% if  messages[0]['role']==system%}{{ raise_exception('System message is not supported in gemma, it would be good to merget the system prompt with first user message')}}{% endif %}{% for message in messages %}{{'<start_of_turn>' + message['role'] + '\n' + message['content'] | trim + '<eos_turn><eos>\n'}}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}"
chat_template = "{{ bos_token }}{% if  messages[0]['role']==system%}{{ raise_exception('System message is not supported in gemma, it would be good to merget the system prompt with first user message')}}{% endif %}{% for message in messages %}{{'<start_of_turn>' + message['role'] + '\n' + message['content'] | trim + '<eos_turn><eos>\n'}}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}"



tokenizer.chat_template = chat_template_for_preprocessing

print(tokenizer.apply_chat_template(sample_data, tokenize=False))

ds = ds.rename_column('conversations', 'messages')

# preprocess dataset {apply chat template}

def preprocess_dataset(row):
    messages = row['messages'] # Dict['str', List[Dict['str', 'str']]]

    # Check if there system message, if yes, merge the system prompt with first user input, since the gemma model does not have system prompt , it trained in this way
    if messages[0]['role'] == 'system':
        system_message = messages[0]['content']
        messages[1]['content'] = system_message + "Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thought}</think>\n\n" + messages[1]['content']
        # after merging, remove the system message from message
        messages.pop(0)

    return {'text': tokenizer.apply_chat_template(messages, tokenize=False)}

ds = ds.map(preprocess_dataset, remove_columns=['messages'])

ds = ds['train'].train_test_split(.1, seed=0)

test_split = ds['test'].train_test_split(0.4, seed=0)

ds['validation'] = test_split['train']
ds['test'] = test_split['test']

ds

from dataclasses import dataclass
from enum import Enum

class ChatToolLMSpecialTokens(Enum):
    tools = '<tools>'
    eos_tools = '</tools>'
    think = '<think>'
    eos_think = '</think>'
    tool_response = '<tool_response>'
    eos_tool_response = '</tool_response>'
    pad_token = '<pad>'
    eos_token = '<eos>'

    @classmethod
    def special_token_list(clss):
        return [cls_token.value for cls_token in clss]

ChatToolLMSpecialTokens.special_token_list()

list(ChatToolLMSpecialTokens)

ChatToolLMSpecialTokens.pad_token.value

# Load tokenizer again so we can add special tokens easly
tokenizer = AutoTokenizer.from_pretrained(model_id,
                                          pad_token=ChatToolLMSpecialTokens.pad_token.value,
                                          eos_token=ChatToolLMSpecialTokens.eos_token.value,
                                          additional_special_tokens=ChatToolLMSpecialTokens.special_token_list())

# set new chat template with bos token for sft auto data processing
tokenizer.chat_template = chat_template

peft_config = LoraConfig(r=64,
                         lora_alpha=128,
                         target_modules=['q_proj', 'v_proj', 'o_proj', 'embed_tokens', 'k_proj', 'lm_head', 'up_proj'], # 'gate_proj', 'up_proj', 'down_proj'],
                         task_type=TaskType.CAUSAL_LM,
                         lora_dropout=0.01)

len(tokenizer) # Total number of tokens

# since we added some special tokenzer , we need to add some random embedding for it
base_model.resize_token_embeddings(len(tokenizer))

from peft import get_peft_model, prepare_model_for_kbit_training

base_model.gradient_checkpointing_enable()

peft_model = prepare_model_for_kbit_training(base_model)
peft_model = get_peft_model(peft_model, peft_config=peft_config)

peft_model.config.use_cache = False

peft_model.print_trainable_parameters()

training_arugment = SFTConfig(output_dir='gemma_function_calling_and_thinking',
                              per_device_train_batch_size=1,
                              per_device_eval_batch_size=1,
                              gradient_accumulation_steps=4,
                              optim="paged_adamw_8bit",
                              logging_first_step=True,
                              logging_dir='runs',
                              learning_rate=1e-4,
                              max_grad_norm=1.0,
                              num_train_epochs=2,
                              warmup_ratio=0.1,
                              lr_scheduler_type='cosine',
                              eval_strategy='steps',
                              save_strategy='steps',
                              report_to='tensorboard',
                              gradient_checkpointing=True,
                              # gradient_checkpointing_kwargs={"use_reentrant": False},
                              packing=False,
                              save_steps=100,
                              eval_steps=10,
                              logging_steps=10,
                              fp16=True,
                              max_seq_length=1500)

trainer =  SFTTrainer(model=peft_model,
                      processing_class=tokenizer,
                      peft_config=peft_config,
                      train_dataset=ds['train'],
                      eval_dataset=ds['validation'].select(range(50)),
                      args=training_arugment)

peft_model.device

trainer.train()

"""## Infernce"""

dataloader = trainer.get_train_dataloader()

for i in dataloader:
    break

tokenizer.decode(tokenizer.all_special_ids)

print(tokenizer.batch_decode(i['input_ids'])[0])

inp = tokenizer.batch_decode(i['input_ids'])[0]
labels = tokenizer.batch_decode(i['labels'])[0]
inp==labels

# len(i['labels'][0])

# print(labels)

# tokenizer.decode(i['input_ids'][0])

##inference

from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer
import torch

from peft import PeftModel
from transformers import AutoModelForCausalLM
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer
import torch
peft_model_id = "Abdulvajid/Llama_Qlora_Reasoning_ToolCalling_Finetuned_4Bit"
# model_id = "meta-llama/Llama-3.2-3B-Instruct"

model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, torch_dtype=torch.float16, device_map='cuda', load_in_4bit=True)
tokenizer = AutoTokenizer.from_pretrained(peft_model_id)
model.eval();

model.eval()

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(peft_model_id)

# from peft import PeftModel

# ft_model = PeftModel.from_pretrained(base_model, "/kaggle/working/peft-dialogue-summary-training-1705417060/checkpoint-1000",torch_dtype=torch.float16,is_trainable=False)



input = """<|begin_of_text|><start_of_turn>human
You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'search_restaurants', 'description': 'Search for restaurants in a specific location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The location to search for restaurants'}, 'cuisine': {'type': 'string', 'description': 'The cuisine type to filter the restaurants'}, 'price_range': {'type': 'integer', 'description': 'The price range of the restaurants (1-4)'}}, 'required': ['location']}}}, {'type': 'function', 'function': {'name': 'generate_random_password', 'description': 'Generate a random password', 'parameters': {'type': 'object', 'properties': {'length': {'type': 'integer', 'description': 'The length of the password'}}, 'required': ['length']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:
<tool_call>
{tool_call}
</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thought}</think>

Im in kerala in india, currently i am in city called malappuram, can you find some restuarant here<eos_turn><eos>
<start_of_turn>model"""

inp = tokenizer.encode(input, return_tensors='pt',add_special_tokens=False).to('cuda')

print(tokenizer.decode(inp[0]))

out = model.generate(inp, max_new_tokens=200)

print(tokenizer.decode(out[0]))

model

print(tokenizer.decode(ds['train'][0]['messages']))

trainer.push_to_hub("Abdulvajid/Llama_Qlora_Reasoning&ToolCalling_Finetuned_4Bit")
# tokenizer.push_to_hub("Abdulvajid/gemma2b_it_toolcalling_finetuned")

from transformers import AutoTokenizer
peft_model_id = "Abdulvajid/Llama_Qlora_Reasoning_ToolCalling_Finetuned_4Bit"
tokenizer = AutoTokenizer.from_pretrained(peft_model_id)

messages = [
    {"role": "user", "content": "I'm in Malappuram, find a restaurant."}
]

template = """{{ bos_token}}{% if  messages[0]['role']==system%}
{{ raise_exception('System message is not supported in gemma, it would be good to merget the system prompt with first user message')}}
{% endif %}
{% if not tools%}
{{ raise_exception('You should provide tools, this model only trained for tool calling with Reasoning')}}
{% endif %}
{% set first_message=True %}
{% for message in messages %}
{% if first_message and tools%}
{{"<start_of_turn>human\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> " ~ (tools | tojson) ~ " </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{tool_call}\n</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thought}</think>\n\n" + message['content'] }}
{% set first_message = False %}
{% else %}
{{'<start_of_turn>' + message['role'] + '\n' + message['content'] | trim + '<eos_turn><eos>\n'}}
{% endif %}
{% endfor %}
{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}"
"""

tokenizer.chat_template = template

tools = [
    {
        "type": "function",
        "function": {
            "name": "search_restaurants",
            "description": "Search for restaurants in a specific location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The location to search for restaurants"
                    },
                    "cuisine": {
                        "type": "string",
                        "description": "The cuisine type to filter the restaurants"
                    },
                    "price_range": {
                        "type": "integer",
                        "description": "The price range of the restaurants (1 = cheap to 4 = very expensive)"
                    }
                },
                "required": ["location"]
            }
        }
    }
]

messages=[
        {"role": "user", "content": "I'm in Malappuram, can you find a restaurant for me?"}
    ]

import json
prompt = tokenizer.apply_chat_template(
    messages,
    tools=tools,  # tool_list is a list of function dicts
    add_generation_prompt=True,
    tokenize=False
)

tokeni

print(prompt)

tokenizer.push_to_hub("Abdulvajid/gemma2b_it_toolcalling_finetuned")

from huggingface_hub import login
login()

tokenizer.chat_template

