{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FpvaAjBuEhw6"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install -U datasets\n",
        "# !pip install -U trl\n",
        "# !pip install -U transformers accelerate\n",
        "# !pip install -U bitsandbytes\n",
        "# # # # # uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFUIwOH3aotx",
        "outputId": "986abc7b-3075-479e-cea0-54eda7185e80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch version:  2.6.0+cu124\n",
            "is gpu enbled:  True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# from dotenv import load_dotenv\n",
        "import os\n",
        "from peft import LoraConfig\n",
        "from peft import TaskType\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from transformers import BitsAndBytesConfig\n",
        "# load_dotenv()\n",
        "\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "\n",
        "print(\"torch version: \", torch.__version__)\n",
        "print('is gpu enbled: ', torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fgr4xXD_GDsp"
      },
      "outputs": [],
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179,
          "referenced_widgets": [
            "9b9a5d4fd16c4348afaa3a47ef769649",
            "407c8edc9ef247f3a49d99560782f480",
            "f64844315dc24e5984c8a9d26810342e",
            "3bdcce6e0cb44a0c96465cef167f4e3f",
            "0c9acfd48beb4b2b89f8bb02754d01cb",
            "147d507bd3ac4f78843d6d02d4889c6f",
            "1f9065dfefde403fab42fe29e4d0cd01",
            "0a8f9c85b3b4457aa9e59aba1bda9471",
            "59d020869a4547b1a1d8d338f48dd978",
            "86c5d07a083f483880cc00007cf8b1cf",
            "dd788705c6e14dca91cd4dff4646fceb"
          ]
        },
        "id": "BwJlUhr0D6y4",
        "outputId": "e16bb191-5d5a-4eee-a624-1635ae633999"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b9a5d4fd16c4348afaa3a47ef769649",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load model and tokenizer\n",
        "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                                  quantization_config=bnb_config,\n",
        "                                                  device_map='auto')\n",
        "# meta-llama/Llama-3.2-3B-Instruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cbBnjI7iaotx"
      },
      "outputs": [],
      "source": [
        "# for name, weights in model.named_parameters():\n",
        "#     print(name)\n",
        "#     print(weights)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j6GXHzbPHCym"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"Jofthomas/hermes-function-calling-thinking-V1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmfZXA-naotx",
        "outputId": "d0255cb9-4814-42db-d981-da1518914a10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'conversations': [{'content': \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'get_stock_price', 'description': 'Get the current stock price of a company', 'parameters': {'type': 'object', 'properties': {'company': {'type': 'string', 'description': 'The name of the company'}}, 'required': ['company']}}}, {'type': 'function', 'function': {'name': 'get_movie_details', 'description': 'Get details about a movie', 'parameters': {'type': 'object', 'properties': {'title': {'type': 'string', 'description': 'The title of the movie'}}, 'required': ['title']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>\",\n",
              "   'role': 'system'},\n",
              "  {'content': 'Hi, can you tell me the current stock price of Apple?',\n",
              "   'role': 'human'},\n",
              "  {'content': \"<think>Alright, the user asked for the current stock price of Apple. Looking at the tools available, there's a function called get_stock_price which exactly fits this request. I'll need to call this function with the parameter company set to 'Apple' to retrieve the information the user is asking for. This seems like the most straightforward and appropriate action to take.\\n</think><tool_call>\\n{'name': 'get_stock_price', 'arguments': {'company': 'Apple'}}\\n</tool_call>\",\n",
              "   'role': 'model'},\n",
              "  {'content': \"<tool_response>\\n{'stock_price': '$150.75'}\\n</tool_response>\",\n",
              "   'role': 'tool'},\n",
              "  {'content': 'The current stock price of Apple is $150.75.', 'role': 'model'},\n",
              "  {'content': \"That's great. Can you also tell me the stock price for Microsoft?\",\n",
              "   'role': 'human'},\n",
              "  {'content': \"<think>Alright, let me try to figure out how to handle this user's request. The user first asked about Apple's stock price, and I responded by calling the get_stock_price function with 'Apple' as the argument. The tool provided the price, and I passed that information back to the user.\\n\\nNow, the user is asking for the stock price of Microsoft. I need to determine the appropriate action here. Looking at the tools available, there's still the get_stock_price function, which is designed to fetch current stock prices. Since the user is requesting another company's stock price, it makes sense to use the same function again but with the company name changed to Microsoft. \\n\\nI shouldn't assume any other tools are needed because the available functions are specifically tailored for stock prices and movie details. In this case, the relevant function is get_stock_price. I should ensure that the function is called with the correct parameter, 'Microsoft' in this case, to retrieve the accurate information the user is seeking. This approach is efficient and aligns directly with the user's request without making unnecessary assumptions or using unrelated tools.\\n</think><tool_call>\\n{'name': 'get_stock_price', 'arguments': {'company': 'Microsoft'}}\\n</tool_call>\",\n",
              "   'role': 'model'},\n",
              "  {'content': \"<tool_response>\\n{'stock_price': '$210.22'}\\n</tool_response>\",\n",
              "   'role': 'tool'},\n",
              "  {'content': 'The current stock price of Microsoft is $210.22.',\n",
              "   'role': 'model'},\n",
              "  {'content': 'Thank you for the information.', 'role': 'human'},\n",
              "  {'content': \"You're welcome! If you have any other questions, feel free to ask.\",\n",
              "   'role': 'model'}]}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPNNJBraaoty",
        "outputId": "642eaf3d-fba7-46a1-9184-c9ded3f3c5ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 3072)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oR3VM6SbHe3A"
      },
      "outputs": [],
      "source": [
        "# ds['train'][12] # sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ucNkwarHaoty"
      },
      "outputs": [],
      "source": [
        "sample_data = ds['train'][0]['conversations']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xmXE3rTlaoty"
      },
      "outputs": [],
      "source": [
        "# we won't use bos token in here , since it's will be added when we set SFTTrainer and data get tokenized, so we set new chat template there with bos token and no other difference\n",
        "chat_template_for_preprocessing = \"{% if  messages[0]['role']==system%}{{ raise_exception('System message is not supported in gemma, it would be good to merget the system prompt with first user message')}}{% endif %}{% for message in messages %}{{'<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<eos_turn><eos>\\n'}}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
        "chat_template = \"{{ bos_token }}{% if  messages[0]['role']==system%}{{ raise_exception('System message is not supported in gemma, it would be good to merget the system prompt with first user message')}}{% endif %}{% for message in messages %}{{'<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<eos_turn><eos>\\n'}}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wPC5WMYEHhk3"
      },
      "outputs": [],
      "source": [
        "tokenizer.chat_template = chat_template_for_preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIGqZHRRaoty",
        "outputId": "e86e22a7-9841-42c4-e605-c5515b5336ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start_of_turn>system\n",
            "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'get_stock_price', 'description': 'Get the current stock price of a company', 'parameters': {'type': 'object', 'properties': {'company': {'type': 'string', 'description': 'The name of the company'}}, 'required': ['company']}}}, {'type': 'function', 'function': {'name': 'get_movie_details', 'description': 'Get details about a movie', 'parameters': {'type': 'object', 'properties': {'title': {'type': 'string', 'description': 'The title of the movie'}}, 'required': ['title']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
            "<tool_call>\n",
            "{tool_call}\n",
            "</tool_call><eos_turn><eos>\n",
            "<start_of_turn>human\n",
            "Hi, can you tell me the current stock price of Apple?<eos_turn><eos>\n",
            "<start_of_turn>model\n",
            "<think>Alright, the user asked for the current stock price of Apple. Looking at the tools available, there's a function called get_stock_price which exactly fits this request. I'll need to call this function with the parameter company set to 'Apple' to retrieve the information the user is asking for. This seems like the most straightforward and appropriate action to take.\n",
            "</think><tool_call>\n",
            "{'name': 'get_stock_price', 'arguments': {'company': 'Apple'}}\n",
            "</tool_call><eos_turn><eos>\n",
            "<start_of_turn>tool\n",
            "<tool_response>\n",
            "{'stock_price': '$150.75'}\n",
            "</tool_response><eos_turn><eos>\n",
            "<start_of_turn>model\n",
            "The current stock price of Apple is $150.75.<eos_turn><eos>\n",
            "<start_of_turn>human\n",
            "That's great. Can you also tell me the stock price for Microsoft?<eos_turn><eos>\n",
            "<start_of_turn>model\n",
            "<think>Alright, let me try to figure out how to handle this user's request. The user first asked about Apple's stock price, and I responded by calling the get_stock_price function with 'Apple' as the argument. The tool provided the price, and I passed that information back to the user.\n",
            "\n",
            "Now, the user is asking for the stock price of Microsoft. I need to determine the appropriate action here. Looking at the tools available, there's still the get_stock_price function, which is designed to fetch current stock prices. Since the user is requesting another company's stock price, it makes sense to use the same function again but with the company name changed to Microsoft. \n",
            "\n",
            "I shouldn't assume any other tools are needed because the available functions are specifically tailored for stock prices and movie details. In this case, the relevant function is get_stock_price. I should ensure that the function is called with the correct parameter, 'Microsoft' in this case, to retrieve the accurate information the user is seeking. This approach is efficient and aligns directly with the user's request without making unnecessary assumptions or using unrelated tools.\n",
            "</think><tool_call>\n",
            "{'name': 'get_stock_price', 'arguments': {'company': 'Microsoft'}}\n",
            "</tool_call><eos_turn><eos>\n",
            "<start_of_turn>tool\n",
            "<tool_response>\n",
            "{'stock_price': '$210.22'}\n",
            "</tool_response><eos_turn><eos>\n",
            "<start_of_turn>model\n",
            "The current stock price of Microsoft is $210.22.<eos_turn><eos>\n",
            "<start_of_turn>human\n",
            "Thank you for the information.<eos_turn><eos>\n",
            "<start_of_turn>model\n",
            "You're welcome! If you have any other questions, feel free to ask.<eos_turn><eos>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.apply_chat_template(sample_data, tokenize=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cBspRVkcaoty"
      },
      "outputs": [],
      "source": [
        "ds = ds.rename_column('conversations', 'messages')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GCt2E9HCaoty"
      },
      "outputs": [],
      "source": [
        "# preprocess dataset {apply chat template}\n",
        "\n",
        "def preprocess_dataset(row):\n",
        "    messages = row['messages'] # Dict['str', List[Dict['str', 'str']]]\n",
        "\n",
        "    # Check if there system message, if yes, merge the system prompt with first user input, since the gemma model does not have system prompt , it trained in this way\n",
        "    if messages[0]['role'] == 'system':\n",
        "        system_message = messages[0]['content']\n",
        "        messages[1]['content'] = system_message + \"Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thought}</think>\\n\\n\" + messages[1]['content']\n",
        "        # after merging, remove the system message from message\n",
        "        messages.pop(0)\n",
        "\n",
        "    return {'text': tokenizer.apply_chat_template(messages, tokenize=False)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vLmtQk9caoty"
      },
      "outputs": [],
      "source": [
        "ds = ds.map(preprocess_dataset, remove_columns=['messages'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qTxzqaERaoty"
      },
      "outputs": [],
      "source": [
        "ds = ds['train'].train_test_split(.1, seed=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "A-nFV2Xpaoty"
      },
      "outputs": [],
      "source": [
        "test_split = ds['test'].train_test_split(0.4, seed=0)\n",
        "\n",
        "ds['validation'] = test_split['train']\n",
        "ds['test'] = test_split['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU-lUeXKgCki",
        "outputId": "bd9b6631-c6a0-4b9c-acf5-917d2dd390de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 3213\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 143\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 214\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6Ny6llDaotz",
        "outputId": "ffd41b68-fd58-47d4-d52c-bc2bb1b8c78c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<tools>',\n",
              " '</tools>',\n",
              " '<think>',\n",
              " '</think>',\n",
              " '<tool_response>',\n",
              " '</tool_response>',\n",
              " '<pad>',\n",
              " '<eos>']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "\n",
        "class ChatToolLMSpecialTokens(Enum):\n",
        "    tools = '<tools>'\n",
        "    eos_tools = '</tools>'\n",
        "    think = '<think>'\n",
        "    eos_think = '</think>'\n",
        "    tool_response = '<tool_response>'\n",
        "    eos_tool_response = '</tool_response>'\n",
        "    pad_token = '<pad>'\n",
        "    eos_token = '<eos>'\n",
        "\n",
        "    @classmethod\n",
        "    def special_token_list(clss):\n",
        "        return [cls_token.value for cls_token in clss]\n",
        "\n",
        "ChatToolLMSpecialTokens.special_token_list()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prIhbErzaotz",
        "outputId": "d5f90451-ae24-49a2-dffc-ff7957e36978"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<ChatToolLMSpecialTokens.tools: '<tools>'>,\n",
              " <ChatToolLMSpecialTokens.eos_tools: '</tools>'>,\n",
              " <ChatToolLMSpecialTokens.think: '<think>'>,\n",
              " <ChatToolLMSpecialTokens.eos_think: '</think>'>,\n",
              " <ChatToolLMSpecialTokens.tool_response: '<tool_response>'>,\n",
              " <ChatToolLMSpecialTokens.eos_tool_response: '</tool_response>'>,\n",
              " <ChatToolLMSpecialTokens.pad_token: '<pad>'>,\n",
              " <ChatToolLMSpecialTokens.eos_token: '<eos>'>]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(ChatToolLMSpecialTokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CBB72Cb8aotz",
        "outputId": "3d5b2734-9bc6-4cec-e5f9-31a9c498f8f9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<pad>'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ChatToolLMSpecialTokens.pad_token.value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZLF0pYezaotz"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer again so we can add special tokens easly\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,\n",
        "                                          pad_token=ChatToolLMSpecialTokens.pad_token.value,\n",
        "                                          eos_token=ChatToolLMSpecialTokens.eos_token.value,\n",
        "                                          additional_special_tokens=ChatToolLMSpecialTokens.special_token_list())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CSLtxW69aotz"
      },
      "outputs": [],
      "source": [
        "# set new chat template with bos token for sft auto data processing\n",
        "tokenizer.chat_template = chat_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mJNp4opRaotz"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(r=64,\n",
        "                         lora_alpha=128,\n",
        "                         target_modules=['q_proj', 'v_proj', 'o_proj', 'embed_tokens', 'k_proj', 'lm_head'], # 'gate_proj', 'up_proj', 'down_proj'],\n",
        "                         task_type=TaskType.CAUSAL_LM,\n",
        "                         lora_dropout=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TO8jmmTNLZOb",
        "outputId": "6895677f-199d-4e71-d546-0db385601c0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "128264"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenizer) # Total number of tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1imdBRfSLVav",
        "outputId": "4a71078c-61b5-4431-99ec-df825eb0b8c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Embedding(128264, 3072)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# since we added some special tokenzer , we need to add some random embedding for it\n",
        "base_model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OijsMZFTJddG",
        "outputId": "ba91217f-30f5-4f82-ddaa-bdd797b81002"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:574: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.embed_tokens', 'lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from peft import get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "base_model.gradient_checkpointing_enable()\n",
        "\n",
        "peft_model = prepare_model_for_kbit_training(base_model)\n",
        "peft_model = get_peft_model(peft_model, peft_config=peft_config)\n",
        "\n",
        "peft_model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkRyh0AGM3nd",
        "outputId": "6139a9b1-0a30-4a50-bb67-78af6f89b0b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 53,511,168 || all params: 3,266,285,568 || trainable%: 1.6383\n"
          ]
        }
      ],
      "source": [
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "RBnbhVoQaotz"
      },
      "outputs": [],
      "source": [
        "training_arugment = SFTConfig(output_dir='gemma_function_calling_and_thinking',\n",
        "                              per_device_train_batch_size=1,\n",
        "                              per_device_eval_batch_size=1,\n",
        "                              gradient_accumulation_steps=4,\n",
        "                              optim=\"paged_adamw_8bit\",\n",
        "                              logging_first_step=True,\n",
        "                              logging_dir='runs',\n",
        "                              learning_rate=1e-4,\n",
        "                              max_grad_norm=1.0,\n",
        "                              num_train_epochs=2,\n",
        "                              warmup_ratio=0.1,\n",
        "                              lr_scheduler_type='cosine',\n",
        "                              eval_strategy='steps',\n",
        "                              save_strategy='steps',\n",
        "                              report_to='tensorboard',\n",
        "                              gradient_checkpointing=True,\n",
        "                              # gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "                              packing=False,\n",
        "                              save_steps=100,\n",
        "                              eval_steps=10,\n",
        "                              logging_steps=10,\n",
        "                              fp16=True,\n",
        "                              max_seq_length=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFxV-n82aot0",
        "outputId": "b3ac0c66-efce-4a87-992a-bf32c513bcda"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "trainer =  SFTTrainer(model=peft_model,\n",
        "                      processing_class=tokenizer,\n",
        "                      peft_config=peft_config,\n",
        "                      train_dataset=ds['train'],\n",
        "                      eval_dataset=ds['validation'].select(range(50)),\n",
        "                      args=training_arugment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN7TqxFqNGpw",
        "outputId": "03925d9c-68c9-4631-bfa1-dbecceedbb76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_model.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "xglAmAB2aot0",
        "outputId": "fd642523-9fed-45d7-c1d2-c80f09faedbd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='149' max='1608' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 149/1608 29:23 < 4:51:42, 0.08 it/s, Epoch 0.18/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.762600</td>\n",
              "      <td>1.775071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.675200</td>\n",
              "      <td>1.608636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.517600</td>\n",
              "      <td>1.396928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.287800</td>\n",
              "      <td>1.107835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.003600</td>\n",
              "      <td>0.817067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.807000</td>\n",
              "      <td>0.702693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.707800</td>\n",
              "      <td>0.652285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.580116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.543500</td>\n",
              "      <td>0.463054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.454100</td>\n",
              "      <td>0.401399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.412700</td>\n",
              "      <td>0.385917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.407100</td>\n",
              "      <td>0.379024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.364500</td>\n",
              "      <td>0.372430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.389300</td>\n",
              "      <td>0.365698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:222: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v1DtTc-aot0"
      },
      "source": [
        "## Infernce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-ihu89Taot0"
      },
      "outputs": [],
      "source": [
        "dataloader = trainer.get_train_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSehE_uCaot0"
      },
      "outputs": [],
      "source": [
        "for i in dataloader:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arm-9gREaot0"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(tokenizer.all_special_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gEw9UFUaot0"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.batch_decode(i['input_ids'])[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ye_4p0bQaot1"
      },
      "outputs": [],
      "source": [
        "inp = tokenizer.batch_decode(i['input_ids'])[0]\n",
        "labels = tokenizer.batch_decode(i['labels'])[0]\n",
        "inp==labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm4uT-3Uaot1"
      },
      "outputs": [],
      "source": [
        "# len(i['labels'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-G2xWveaot1"
      },
      "outputs": [],
      "source": [
        "# print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhXMeJYyaot1"
      },
      "outputs": [],
      "source": [
        "# tokenizer.decode(i['input_ids'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0cAffJcaot1"
      },
      "outputs": [],
      "source": [
        "##inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLBYBwO9aot1"
      },
      "outputs": [],
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xadkO_Mxaot1"
      },
      "outputs": [],
      "source": [
        "# Inference\n",
        "peft_model_id = \"/content/gemma_function_calling_and_thinking/checkpoint-400\"\n",
        "\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, torch_dtype=torch.float16, device_map='cuda')\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZqfHPF0Hf5P"
      },
      "outputs": [],
      "source": [
        "# from peft import PeftModel\n",
        "\n",
        "# ft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-dialogue-summary-training-1705417060/checkpoint-1000\",torch_dtype=torch.float16,is_trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZcpKzNsaot2"
      },
      "outputs": [],
      "source": [
        "input = \"\"\"<start_of_turn>human\n",
        "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'search_restaurants', 'description': 'Search for restaurants in a specific location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The location to search for restaurants'}, 'cuisine': {'type': 'string', 'description': 'The cuisine type to filter the restaurants'}, 'price_range': {'type': 'integer', 'description': 'The price range of the restaurants (1-4)'}}, 'required': ['location']}}}, {'type': 'function', 'function': {'name': 'generate_random_password', 'description': 'Generate a random password', 'parameters': {'type': 'object', 'properties': {'length': {'type': 'integer', 'description': 'The length of the password'}}, 'required': ['length']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
        "<tool_call>\n",
        "{tool_call}\n",
        "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thought}</think>\n",
        "\n",
        "I'm in New York and I'm craving Italian food. Can you find a restaurant for me?<eos_turn><eos>\n",
        "<start_of_turn>model\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ3N6_-Zaot2"
      },
      "outputs": [],
      "source": [
        "inp = tokenizer.encode(input, return_tensors='pt',add_special_tokens=False).to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZEkFyeBaot2"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(inp[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ug-GG1uaot3"
      },
      "outputs": [],
      "source": [
        "out = model.generate(inp, max_new_tokens=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7inG3OMaot3"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(out[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wub0OvzJaot3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMFCyek7aot3"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(ds['train'][0]['messages']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHstFBvRaot3"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(\"your-username/finetuned-gpt2\")\n",
        "tokenizer.push_to_hub(\"your-username/finetuned-gpt2\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a8f9c85b3b4457aa9e59aba1bda9471": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c9acfd48beb4b2b89f8bb02754d01cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "147d507bd3ac4f78843d6d02d4889c6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f9065dfefde403fab42fe29e4d0cd01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bdcce6e0cb44a0c96465cef167f4e3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86c5d07a083f483880cc00007cf8b1cf",
            "placeholder": "​",
            "style": "IPY_MODEL_dd788705c6e14dca91cd4dff4646fceb",
            "value": " 2/2 [00:37&lt;00:00, 17.17s/it]"
          }
        },
        "407c8edc9ef247f3a49d99560782f480": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_147d507bd3ac4f78843d6d02d4889c6f",
            "placeholder": "​",
            "style": "IPY_MODEL_1f9065dfefde403fab42fe29e4d0cd01",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "59d020869a4547b1a1d8d338f48dd978": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86c5d07a083f483880cc00007cf8b1cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b9a5d4fd16c4348afaa3a47ef769649": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_407c8edc9ef247f3a49d99560782f480",
              "IPY_MODEL_f64844315dc24e5984c8a9d26810342e",
              "IPY_MODEL_3bdcce6e0cb44a0c96465cef167f4e3f"
            ],
            "layout": "IPY_MODEL_0c9acfd48beb4b2b89f8bb02754d01cb"
          }
        },
        "dd788705c6e14dca91cd4dff4646fceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f64844315dc24e5984c8a9d26810342e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a8f9c85b3b4457aa9e59aba1bda9471",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59d020869a4547b1a1d8d338f48dd978",
            "value": 2
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
